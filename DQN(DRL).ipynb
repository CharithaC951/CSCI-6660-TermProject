{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e15404-7718-4d13-a470-179336599df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\dnave\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: pycocotools in c:\\users\\dnave\\anaconda3\\lib\\site-packages (2.0.8)\n",
      "Requirement already satisfied: gym in c:\\users\\dnave\\anaconda3\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from pycocotools) (3.9.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dnave\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow pycocotools gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6eb101-98f6-43f5-b8a0-8c7f432b0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from pycocotools.coco import COCO\n",
    "import random\n",
    "import gym\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c76a53f-c6af-4532-a7a7-58734f93a417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.73s)\n",
      "creating index...\n",
      "index created!\n",
      "Image file path: C:\\Users\\dnave\\Downloads\\COCO Dataset\\coco2017\\val2017\\000000397133.jpg\n",
      "Image loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Define the root directory for the COCO dataset\n",
    "dataDir = r'C:\\Users\\dnave\\Downloads\\COCO Dataset\\coco2017'\n",
    "\n",
    "# Specify the data type (train or val)\n",
    "dataType = 'val2017'  # Use validation set; change to 'train2017' for training set\n",
    "\n",
    "# Define the path to the annotation file (instances for the selected data type)\n",
    "annFile = f'{dataDir}\\\\annotations\\\\instances_{dataType}.json'\n",
    "\n",
    "# Initialize COCO API for COCO annotations\n",
    "coco = COCO(annFile)\n",
    "\n",
    "# Get the image ids\n",
    "imgIds = coco.getImgIds()\n",
    "\n",
    "# Load the first image (you can change the index to load another image)\n",
    "img = coco.loadImgs(imgIds[0])[0]\n",
    "\n",
    "# Load image and annotations (example for one image)\n",
    "imgFile = f\"{dataDir}\\\\{dataType}\\\\{img['file_name']}\"\n",
    "\n",
    "# Debugging: Print the file path to ensure it's correct\n",
    "print(f\"Image file path: {imgFile}\")\n",
    "\n",
    "# Try to load the image\n",
    "try:\n",
    "    image = tf.keras.preprocessing.image.load_img(imgFile)\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    print(\"Image loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {imgFile}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5a7a18-df56-432c-9cf0-a4d92d3338df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    }
   ],
   "source": [
    "# Load the ResNet50 model pre-trained on ImageNet without the top (classification) layer\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Create a new model that includes the base model and the output layer\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Resize the image to 224x224 for ResNet50\n",
    "image_resized = tf.image.resize(image, (224, 224))\n",
    "image_resized = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "# Extract features from the image using the pre-trained ResNet50 model\n",
    "features = model.predict(image_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f41b9dd9-2160-48be-b318-ffeffd1b84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network (simplified for object detection)\n",
    "class DQNModel(tf.keras.Model):\n",
    "    def __init__(self, action_space):\n",
    "        super(DQNModel, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(action_space, activation='linear')  # Action space for bounding box and class\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.flatten(state)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "# Define the DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)  # Experience replay memory\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQNModel(action_size)\n",
    "        self.target_model = DQNModel(action_size)  # Target model for stability\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)  # Exploration\n",
    "        q_values = self.model(state)\n",
    "        return np.argmax(q_values[0])  # Exploitation\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.max(self.target_model(next_state))\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = self.model(state)\n",
    "                loss = tf.keras.losses.MSE(q_values[0][action], target)\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        # Update epsilon for exploration-exploitation balance\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # Update target model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a970de7b-5c54-4892-9eee-ea3295bc5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(action):\n",
    "    # Define reward function based on the accuracy of bounding box prediction\n",
    "    # Example: +1 for correct detection, -1 for false positive, and 0 for no detection\n",
    "    return random.choice([1, 0, -1])  # Simplified example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ded8186-2ddb-4f5e-a866-d94b786024d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Total Reward: 1\n",
      "Episode 2/1000, Total Reward: 0\n",
      "Episode 3/1000, Total Reward: -1\n",
      "Episode 4/1000, Total Reward: -1\n",
      "Episode 5/1000, Total Reward: 1\n",
      "Episode 6/1000, Total Reward: 0\n",
      "Episode 7/1000, Total Reward: 0\n",
      "Episode 8/1000, Total Reward: 1\n",
      "Episode 9/1000, Total Reward: 1\n",
      "Episode 10/1000, Total Reward: 0\n",
      "Episode 11/1000, Total Reward: 0\n",
      "Episode 12/1000, Total Reward: -1\n",
      "Episode 13/1000, Total Reward: -1\n",
      "Episode 14/1000, Total Reward: -1\n",
      "Episode 15/1000, Total Reward: 1\n",
      "Episode 16/1000, Total Reward: -1\n",
      "Episode 17/1000, Total Reward: -1\n",
      "Episode 18/1000, Total Reward: 0\n",
      "Episode 19/1000, Total Reward: 0\n",
      "Episode 20/1000, Total Reward: -1\n",
      "Episode 21/1000, Total Reward: -1\n",
      "Episode 22/1000, Total Reward: -1\n",
      "Episode 23/1000, Total Reward: 1\n",
      "Episode 24/1000, Total Reward: -1\n",
      "Episode 25/1000, Total Reward: 0\n",
      "Episode 26/1000, Total Reward: -1\n",
      "Episode 27/1000, Total Reward: -1\n",
      "Episode 28/1000, Total Reward: 0\n",
      "Episode 29/1000, Total Reward: 1\n",
      "Episode 30/1000, Total Reward: 0\n",
      "Episode 31/1000, Total Reward: 1\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Mean_device_/job:localhost/replica:0/task:0/device:CPU:0}} Invalid reduction dimension (-1 for input with 0 dimension(s) [Op:Mean] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# End the episode (simplified)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[1;32m---> 23\u001b[0m agent\u001b[38;5;241m.\u001b[39mreplay(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     24\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     25\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     49\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state)\n\u001b[1;32m---> 50\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMSE(q_values[\u001b[38;5;241m0\u001b[39m][action], target)\n\u001b[0;32m     51\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\losses\\losses.py:1303\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m   1301\u001b[0m y_true \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_true, dtype\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   1302\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001b[1;32m-> 1303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mmean(ops\u001b[38;5;241m.\u001b[39msquare(y_true \u001b[38;5;241m-\u001b[39m y_pred), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\numpy.py:6182\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(x, axis, keepdims)\u001b[0m\n\u001b[0;32m   6180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[0;32m   6181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Mean(axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims)\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[1;32m-> 6182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mmean(x, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py:573\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(x, axis, keepdims)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    572\u001b[0m     result_dtype \u001b[38;5;241m=\u001b[39m ori_dtype\n\u001b[1;32m--> 573\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\n\u001b[0;32m    574\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcast(x, compute_dtype), axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims\n\u001b[0;32m    575\u001b[0m )\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(output, result_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Mean_device_/job:localhost/replica:0/task:0/device:CPU:0}} Invalid reduction dimension (-1 for input with 0 dimension(s) [Op:Mean] name: "
     ]
    }
   ],
   "source": [
    "# Define state and action sizes (simplified for object detection)\n",
    "state_size = features.shape[1:]  # Feature map from CNN\n",
    "action_size = 4  # Example: bounding box (x, y, width, height) and class prediction\n",
    "\n",
    "# Initialize the agent\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Train the agent\n",
    "for episode in range(1000):  # 1000 episodes (example)\n",
    "    state = np.expand_dims(features, axis=0)  # Initial state (image feature map)\n",
    "    done = False\n",
    "    time = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        # Simulate taking an action: output bounding box (simplified for illustration)\n",
    "        reward = calculate_reward(action)  # Define reward function based on accuracy of prediction\n",
    "        next_state = state  # Update state based on new bounding box (not implemented here)\n",
    "        done = True  # End the episode (simplified)\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.replay(batch_size=32)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode+1}/{1000}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996329a7-c532-435e-92d4-f09b10a9ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Define the agent (use the DQNAgent class from previous code)\n",
    "# Assume `DQNAgent` has been defined earlier as it was in your previous code\n",
    "\n",
    "# For the sake of simplicity, we'll assume that the agent has been trained\n",
    "# and we are ready to test it.\n",
    "\n",
    "def detect_object_in_image(agent, image_path, coco, dataDir, dataType):\n",
    "    \"\"\"\n",
    "    Given an image path, use the trained DQN agent to detect an object.\n",
    "    The function assumes the agent can predict the bounding box and class.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Preprocess the image to match the input expected by the CNN (state_size)\n",
    "    img_resized = cv2.resize(img_rgb, (224, 224))  # Resize to the expected input size\n",
    "    img_array = np.array(img_resized) / 255.0  # Normalize pixel values\n",
    "\n",
    "    # Add batch dimension to the image\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Get the features from the CNN (This is a placeholder, you can replace it with your CNN model)\n",
    "    features = img_array  # Replace with the actual CNN output features if needed\n",
    "\n",
    "    # Set the initial state (features from the CNN)\n",
    "    state = np.expand_dims(features, axis=0)  # Expand dims for batch size\n",
    "\n",
    "    # Use the DQN agent to predict the action (bounding box + class)\n",
    "    action = agent.act(state)  # This will give us the bounding box and class prediction\n",
    "\n",
    "    # Extract the predicted bounding box and class\n",
    "    predicted_bbox = action[:4]  # Assuming action contains (x, y, width, height, class)\n",
    "    predicted_class = action[4]  # Predicted class label\n",
    "\n",
    "    # Get the class label for the predicted class\n",
    "    category = coco.loadCats(predicted_class)[0]['name']\n",
    "\n",
    "    # Calculate the coordinates of the bounding box\n",
    "    x, y, w, h = predicted_bbox\n",
    "    x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "\n",
    "    # Draw the bounding box and label on the image\n",
    "    cv2.rectangle(img_rgb, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    cv2.putText(img_rgb, f\"{category}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the result\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the result (optional)\n",
    "    cv2.imwrite('detected_object.jpg', cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# Example of how to use this function:\n",
    "image_path = r'C:\\path_to_image\\image.jpg'  # Update with your image path\n",
    "dataDir = r'C:\\Users\\dnave\\Downloads\\COCO Dataset\\coco2017'  # COCO dataset path\n",
    "dataType = 'val2017'  # Validation data type\n",
    "annFile = f'{dataDir}\\\\annotations\\\\instances_{dataType}.json'\n",
    "coco = COCO(annFile)\n",
    "\n",
    "# Assuming the agent is already trained, use the detect_object_in_image function\n",
    "detect_object_in_image(agent, image_path, coco, dataDir, dataType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535942ea-7a65-4edc-849f-44a5e9fbf7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def detect_object_in_image(agent, image_path, coco, dataDir, dataType):\n",
    "    \"\"\"\n",
    "    Given an image path, use the trained DQN agent to detect an object.\n",
    "    The function assumes the agent can predict the bounding box and class.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the image exists\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Error: Image file not found at {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image from {image_path}\")\n",
    "        return\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Preprocess the image to match the input expected by the CNN (state_size)\n",
    "    img_resized = cv2.resize(img_rgb, (224, 224))  # Resize to the expected input size\n",
    "    img_array = np.array(img_resized) / 255.0  # Normalize pixel values\n",
    "\n",
    "    # Add batch dimension to the image\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Get the features from the CNN (This is a placeholder, you can replace it with your CNN model)\n",
    "    features = img_array  # Replace with the actual CNN output features if needed\n",
    "\n",
    "    # Set the initial state (features from the CNN)\n",
    "    state = np.expand_dims(features, axis=0)  # Expand dims for batch size\n",
    "\n",
    "    # Use the DQN agent to predict the action (bounding box + class)\n",
    "    action = agent.act(state)  # This will give us the bounding box and class prediction\n",
    "\n",
    "    # Extract the predicted bounding box and class\n",
    "    predicted_bbox = action[:4]  # Assuming action contains (x, y, width, height, class)\n",
    "    predicted_class = action[4]  # Predicted class label\n",
    "\n",
    "    # Get the class label for the predicted class\n",
    "    category = coco.loadCats(predicted_class)[0]['name']\n",
    "\n",
    "    # Calculate the coordinates of the bounding box\n",
    "    x, y, w, h = predicted_bbox\n",
    "    x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "\n",
    "    # Draw the bounding box and label on the image\n",
    "    cv2.rectangle(img_rgb, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    cv2.putText(img_rgb, f\"{category}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the result\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the result (optional)\n",
    "    cv2.imwrite('detected_object.jpg', cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452ecf8-8cd9-4efe-9463-aee11f6ff08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Define the DQN agent (simplified for object classification)\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.model = self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        # Simple neural network for DQN model (based on state input)\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, input_shape=(self.state_size,), activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='softmax')  # Action size = number of classes\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        # Choose action based on the policy (epsilon-greedy, for example)\n",
    "        action_probs = self.model.predict(state)\n",
    "        return np.argmax(action_probs)  # Return the class with highest probability\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        # Sample a batch of experiences from memory and train the model\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = np.random.choice(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            target = reward  # Simplified target (in practice, use Bellman equation)\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target  # Update target for the selected action\n",
    "            \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "# Define the function to preprocess the image\n",
    "def preprocess_image(image_path, model):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, (224, 224))  # Resize to the expected input size\n",
    "    img_array = np.array(img_resized) / 255.0  # Normalize pixel values\n",
    "    \n",
    "    # Add batch dimension and pass through CNN (ResNet50)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    features = model.predict(img_array)  # CNN feature extraction\n",
    "    return features\n",
    "\n",
    "# Initialize the ResNet50 model for feature extraction\n",
    "cnn_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# COCO dataset paths\n",
    "dataDir = 'C:/Users/dnave/Downloads/COCO Dataset/coco2017'\n",
    "dataType = 'val2017'\n",
    "annFile = f'{dataDir}/annotations/instances_{dataType}.json'\n",
    "coco = COCO(annFile)\n",
    "\n",
    "# Initialize DQN agent\n",
    "state_size = cnn_model.output.shape[1]  # Feature size from CNN (e.g., 2048 for ResNet50)\n",
    "action_size = 80  # COCO has 80 object categories\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Define the reward function (simplified)\n",
    "def calculate_reward(predicted_class, true_class):\n",
    "    return 1 if predicted_class == true_class else -1\n",
    "\n",
    "# Function to detect object in the image using the trained DQN agent\n",
    "def detect_object_in_image(agent, image_path, coco, dataDir, dataType):\n",
    "    # Preprocess the image\n",
    "    features = preprocess_image(image_path, cnn_model)\n",
    "    \n",
    "    # Use the DQN agent to predict the class of the object in the image\n",
    "    state = np.expand_dims(features, axis=0)  # Add batch dimension\n",
    "    predicted_class = agent.act(state)  # Predicted class (index of the object)\n",
    "    \n",
    "    # Get the class name\n",
    "    category = coco.loadCats(predicted_class)[0]['name']\n",
    "    \n",
    "    # Display the results\n",
    "    print(f\"Predicted Class: {category}\")\n",
    "    \n",
    "    # Load and display the image with the predicted label\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    cv2.putText(img_rgb, f\"Predicted: {category}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "image_path = 'C:/path_to_image/000000579970.jpg'  # Replace with your image path\n",
    "detect_object_in_image(agent, image_path, coco, dataDir, dataType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683ada8-4187-49dd-9d8a-b2180cdc3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_path, model):\n",
    "    print(f\"Loading image from path: {image_path}\")\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Error: Image not found or cannot be read from {image_path}\")\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    img_resized = cv2.resize(img_rgb, (224, 224))  # Resize to the expected input size\n",
    "    img_array = np.array(img_resized) / 255.0  # Normalize pixel values\n",
    "    \n",
    "    # Add batch dimension and pass through CNN (ResNet50)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    features = model.predict(img_array)  # CNN feature extraction\n",
    "    return features\n",
    "\n",
    "# Example usage:\n",
    "image_path = 'C:Users/dnave/Downloads/COCO Dataset/val2017/000000579970.jpg'  \n",
    "try:\n",
    "    features = preprocess_image(image_path, cnn_model)\n",
    "    print(\"Image processed successfully.\")\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c8e598-8aa3-418d-a05a-40ffa84e0a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
